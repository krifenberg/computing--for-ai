{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/Practicum_AI_Logo.white_outline.svg' width=250 alt='Practicum AI logo'> <img src='https://github.com/PracticumAI/practicumai.github.io/blob/main/images/icons/practicumai_beginner.png?raw=true' align='right' width=50>\n",
    "***\n",
    "# *Practicum AI:* Deep Learning - MNIST Classifier\n",
    "\n",
    "This exercise adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 2.07, page 92).\n",
    "\n",
    "### MNIST Handwritten Digit Classification Dataset\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) (Modified National Institue of Standards and Technology) training dataset contains 60,000 28Ã—28 pixel grayscale images of handwritten single digits between 0 and 9, with an additional 10,000 images available for testing.  All images in this dataset have been normalized and centered.\n",
    "\n",
    "The MNIST dataset is frequently used in machine learning research and has become a standard benchmark for image classification models. Top-performing models often achieve a classification accuracy above 99%, with an error rate between 0.4 % and 0.2% on the hold-out test dataset.\n",
    "\n",
    "In this exercise, you will implement a deep neural network (multi-layer) capable of classifying these images of handritten digits into one of 10 classes. \n",
    "\n",
    "#### 1. Import libraries\n",
    "\n",
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "# Import Keras libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the MNIST dataset\n",
    "\n",
    "Import the MNIST dataset from the [Keras module](https://keras.io/api/datasets/mnist/). The `train_features` and `test_features` variables contain the training and test images while `train_labels` and `test_labels` contain the corresponding labels for each item in those datasets.  \n",
    "\n",
    "```python\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_features,train_labels), (test_features,test_labels) = mnist.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_features, train_labels), (test_features, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normalize the data\n",
    "\n",
    "Normalize the data by scaling the images so their values are between 0 and 1.\n",
    "\n",
    "```python\n",
    "train_features, test_features = train_features / 255.0, test_features / 255.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "train_features, test_features = train_features / 255.0, test_features / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build the sequential model\n",
    "\n",
    "Using the Sequential API, build your model according to the following spec:\n",
    "\n",
    "* First, a flattened layer to unroll 28x28 pixel images into a single array of 782. The model should use the input_shape in the function argument to set the input size in the first layer.\n",
    "* A dense hidden layer with 50 units (neurons) and ReLU activation functions.\n",
    "* A dense hidden layer with 20 units and ReLU activation functions.\n",
    "* A dense output layer with 10 units and the softmax activation function.\n",
    "\n",
    "Your completed neural network should have four layers. Feel free to experiment with different architectures and build your own model.\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(units = 50, activation = 'relu'))\n",
    "model.add(Dense(units = 20 , activation = 'relu'))\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(units = 50, activation = 'relu'))\n",
    "model.add(Dense(units = 20 , activation = 'relu'))\n",
    "model.add(Dense(units = 10, activation = 'softmax')) #based on the categories 0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compile the model\n",
    "\n",
    "To `compile` the model, you need to specify an optimizer, a loss function, and a metric to judge the performance of your model.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inspect the model configuration using the summary function\n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                39250     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,480\n",
      "Trainable params: 40,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Code it!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary indicates that this model has 40,480 parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Fit the model to the training data\n",
    "\n",
    "Now train the model on the MNIST dataset, using the `fit` method. Set training to run for 50 epochs.\n",
    "\n",
    "```python\n",
    "model.fit(train_features, train_labels, epochs=50)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 135/1875 [=>............................] - ETA: 1s - loss: 1.1885 - accuracy: 0.6315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 14:54:23.028657: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3258 - accuracy: 0.9039\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1489 - accuracy: 0.9564\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.9655\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0908 - accuracy: 0.9725\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0769 - accuracy: 0.9764\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0664 - accuracy: 0.9795\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0576 - accuracy: 0.9821\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0500 - accuracy: 0.9840\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0453 - accuracy: 0.9857\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0398 - accuracy: 0.9872\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0363 - accuracy: 0.9884\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0334 - accuracy: 0.9886\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0287 - accuracy: 0.9907\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0267 - accuracy: 0.9910\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0239 - accuracy: 0.9920\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0215 - accuracy: 0.9931\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0211 - accuracy: 0.9928\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0197 - accuracy: 0.9933\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0200 - accuracy: 0.9934\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0166 - accuracy: 0.9944\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0161 - accuracy: 0.9947\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0148 - accuracy: 0.9955\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0124 - accuracy: 0.9957\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0123 - accuracy: 0.9959\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0129 - accuracy: 0.9958\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0136 - accuracy: 0.9956\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0111 - accuracy: 0.9963\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0115 - accuracy: 0.9961\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0105 - accuracy: 0.9962\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0113 - accuracy: 0.9963\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0091 - accuracy: 0.9968\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0093 - accuracy: 0.9966\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0102 - accuracy: 0.9967\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0088 - accuracy: 0.9971\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0089 - accuracy: 0.9970\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0095 - accuracy: 0.9967\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0088 - accuracy: 0.9971\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0075 - accuracy: 0.9976\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0094 - accuracy: 0.9967\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0092 - accuracy: 0.9969\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9981\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0077 - accuracy: 0.9972\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0093 - accuracy: 0.9968\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9980\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0086 - accuracy: 0.9973\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0079 - accuracy: 0.9974\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9979\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0070 - accuracy: 0.9975\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0082 - accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2abac1d956a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "model.fit(train_features, train_labels, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Evaluate the model\n",
    "\n",
    "Finally, evaluate the performance of your model on the test set, by calling the model's `evaluate()` method.\n",
    "\n",
    "```python\n",
    "model.evaluate(test_features, test_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19453167915344238, 0.972599983215332]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Model predictions\n",
    "\n",
    "Let's see how the model performs on some randomly selected images.  Are its predictions correct?  \n",
    "\n",
    "Randomly select an image from the test dataset, in this case the 200th image.\n",
    "\n",
    "```python\n",
    "loc = 200\n",
    "test_image = test_features[loc]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "loc = 200 \n",
    "test_image = test_features[loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the shape of the image.\n",
    "\n",
    "```python\n",
    "test_image.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our image is 28x28 pixels. However, the model needs not just the size of the image but the number of channels as well. A simple call to the `reshape()` method fixes that problem. \n",
    "\n",
    "```python\n",
    "test_image = test_image.reshape(1,28,28)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "test_image = test_image.reshape(1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the model's `predict()` method, assign the output to result, and then view its contents.\n",
    "\n",
    "```python\n",
    "result = model.predict(test_image)\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "result = model.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the model has returned the probability of 10 predictions, with the highest one being the most likely.  Use the `argmax` function to see the model's prediction.\n",
    "\n",
    "```python\n",
    "result.argmax()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "result.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the prediction, check the label of the corresponding image.\n",
    "\n",
    "```python\n",
    "test_labels[loc]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code it!\n",
    "test_labels[loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, visualize the image with pyplot.\n",
    "\n",
    "```python\n",
    "plt.imshow(test_features[loc])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2abadebadd30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code it!\n",
    "plt.imshow(test_features[loc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
